# Suttton 強化学習

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

## 章立

1. 序

    **(テーブル形式の解法)**

2. 多腕バンディット問題
3. 有限マルコフ決定過程
4. 動的計画法
5. モンテカルロ法
6. TD学習
7. nステップ・ブーストラップ法
8. テーブル形式手法におけるプランニングと学習

   **(近似による解法)**

9. 近似を用いた方策オン型予測
10. 関数近似を用いた方策オン型制御
11. 近似を用いた方策オフ型手法
12. 適格度トレース
13. 方策勾配法

    **(さらに深く)**

14. 心理学
15. 神経科学
16. 応用と事例紹介
17. 強化学習のこれから

## (テーブル形式の解法)

### 1 序

- 強化学習というアプローチは，相互作用に基づく目標志向型の学習

- 1-1. 強化学習
  - 強化学習では、数値化された報酬信号を最大化するために何をすべきかを学習する
  - 試行錯誤と遅延報酬
  - 教師なし学習とは、報酬を最大化することと、各利他構造を見つけ出すわけではないこと
  - 強化学習にあって他にない困難な課題は、探索と活用のトレードオフ
  - 強化学習で興味深い特徴は、ほかの工学分野理学分野との相互作用

- 1-2. 強化学習の例
  - 意思決定を伴うエージェントとその環境との相互作用、状況が不確かであったとしても、目標を達成しようとする
  
- 1-3.強化学習の構成要素
  - 方策、報酬信号、価値観数、モデル
  - 方策:あるエージェントのふるまいを定義、環境で知覚している状態からとるべき行動
  - 報酬:強化学習の問題において目標を定義
  - 価値関数：長期的に何が良いかをあらわすもの
  - モデル：環境の挙動を模倣する、次状態と報酬の予測
  - モデルベースとモデルフリー
    - モデルベース：モデルとプランニングで強化学習を解く
    - モデルフリー：試行錯誤によって問題を解く
  - 進化計算では
    - 価値関数は評価しない。静的な方策空間を探索する。
    - 方策空間が小さければ問題ない。
    - どの状態をとったか、どの行動をとったかは意識されない
    - 途中の行動は評価されないで、最終結果のみ利用

- 1-4.三目並べ
  - greedyと探索
  - TD学習　$V(S_t)←V(S_t)+α[V(S_{t+1})-V(S_t)]$

- 1-7. 強化学習の成り立ち
  - ２つある
    - 試行錯誤に関する動物の学習心理学
    - 最適制御問題、価値観数、動的計画法を用いた解法
  - その他主な研究
    - 1894 Morgan 動物の行動の本質、学習、試行錯誤。
      - 同じ状況でとられる反応のなかで、他の条件が同じならその状況と反応は結びつきやすい
      - 満足感・不快感の大きさでも、結びつきは大きくなる
    - 1927 Pavlov 強化子最適
    - 1933 Thomas 試行錯誤の学習をした電気機械
    - 1960-70 試行錯誤研究は稀有になった。教師あり学習との混同
    - 学習オートマトン、バンディット問題
    - 1978 Sutton TD学習に基づいた古典的条件付けの心理学モデルを作る
    - 1981 Minsky TD学習と試行錯誤を組み合わせたactor-critic手法を開発
    - 1989 Watkins Q学習

### 2 多腕バンディット問題

- 2-1. k本腕バンディット問題
  - 2つ以上の状態を持たない単純な設定で強化学習の評価的な側面を見る
  - 行動選択は、スロットマシンのレバーを1つ引く。獲得利益を最大化する
  - 行動の価値は、報酬の期待値

- 2-2. 行動価値手法
  - 推定値は、行動が選択された時の平均報酬
  - ε-greedy法：基本即時報酬最大で、小さい確率で全行動からランダム選ぶ
  - 完全greedyでは最適解にたどり着けないことがある。特に価値が
  - 非定常な問題には対処できない
- 2-4. 逐次的実装：逐次価値を更新できるように計算を簡略化
- 2-5. 非定常問題：遠い過去より、新しい報酬に老籾をつける。指数直近性加重平均
- 2-6. 楽観的初期値：極端に報酬をもらえる初期値を設定して探索を促す
- 2-7. 上限信頼区間行動選択：不確実性、分散の程度を表す上界を設定
- 2-8. 勾配バンディットアルゴリズムを設定：ほかの行動との相対的評価のため、ソフトマックス関数を使う
- 2-9. 連想探：次の状況にも影響を与える
- 2-10.まとめ
  - 探索と活用のバランスをとるGittins指数と呼ばれる特殊な行動価値として研究される。ベイズ手法の一例


### 3　有限マルコフ過程

- 3-1. エージェントと環境の境界
  - エージェント：学習者
  - 環境：エージェントと相互作用する外側すべてのもの
  - 状態、行動、報酬
  - 有限MDPでは、状態、行動、報酬はすべて有限個の要素を持つ
  - マルコフ決定過程では、環境のダイナミクスの特徴はすべて確率で決まる
  - マルコフ性...状態と報酬は、直前の状態と行動のみに依存する
  - エージェントと環境の境界は、エージェントが自在に制御できるか
- 3-2~
  - 目標は報酬で定式化
  - エピソードは、期待収益、割引の概念で表される
  - 方策：状態からとりうる行動への選択確率の写像
  - 価値観数：特定の状態にいることがどれほどよいか。期待収益で定式化される
    - 状態価値関数

    - 行動価値関数
    - ![alt text](image-4.png)
  - ベルマン方程式:行動をとる確率×期待収益
  - ![alt text](image.png)
  - ![alt text](image-1.png)
  - 最適行動価値関数
  - 最適方策

### 4 動的計画法

- 動的計画法とは、環境の完璧なモデルがマルコフ決定過程として与えられた場合に、最適方策を計算できるような一群のアルゴリズム
- 方策評価
  - 反復方策評価
- 方策改善
  - より良い方策を見つける。sでaをとり、それ以外は現在の方策で言った場合に改善するか否かを確かめる
- 以上、方策評価、方策反復を繰り返すことで最適方策を探す手法を方策反復と呼ぶ
- 反復のたびに方策評価が必要であり、長い反復計算が必要
- 価値反復：方策評価を1回の掃き出しのみで打ち切る

### 5 モンテカルロ法

- モンテカルロ法は、サンプル収益の平均化にもとづき強化学習を解く。経験、シミュレーションによる環境との相互作用から得られる状態、行動、報酬のサンプル系列を使う
- 状態を訪問した際の収益の平均値として推定